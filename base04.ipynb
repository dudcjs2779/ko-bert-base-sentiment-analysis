{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dudcj\\OneDrive\\Desktop\\AI_Study\\FastCampus\\DeepLearning\\Study01\\ko-bert-base-sentiment-analysis\\.env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from datasets import Dataset, DatasetDict\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "from sklearn.model_selection import train_test_split # train test 를 나누기 위한 라이브러리\n",
    "from sklearn.metrics import accuracy_score # 정확도 계산 라이브러리\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU 메모리 초기화\n",
    "def gpu_clear():\n",
    "    device = cuda.get_current_device(); \n",
    "    device.reset()\n",
    "    !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed 고정\n",
    "import random\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "def random_seed(seed_num):\n",
    "    torch.manual_seed(seed_num)\n",
    "    torch.cuda.manual_seed(seed_num)\n",
    "    torch.cuda.manual_seed_all(seed_num)\n",
    "    np.random.seed(seed_num)\n",
    "    cudnn.benchmark = False\n",
    "    cudnn.deterministic = True\n",
    "    random.seed(seed_num)\n",
    "    \n",
    "random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data load for Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_combine_json_files(folder_path):\n",
    "    all_data = []\n",
    "\n",
    "    # 폴더와 하위 폴더를 순회하면서 JSON 파일 찾기\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file_name in files:\n",
    "            if file_name.endswith('.json'):\n",
    "                file_path = os.path.join(root, file_name)\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    json_data = json.load(file)\n",
    "                    all_data.extend(json_data)  # 리스트를 확장하여 데이터 추가\n",
    "\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_path = '../data/train'\n",
    "# test_path = '../data/validation'\n",
    "\n",
    "# train_datas = load_and_combine_json_files(train_path)\n",
    "# test_datas = load_and_combine_json_files(test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = pd.DataFrame(train_datas)[['RawText', 'GeneralPolarity']]\n",
    "# train.dropna(subset=['GeneralPolarity'], inplace=True)\n",
    "# train['GeneralPolarity'] = train['GeneralPolarity'].astype(int)\n",
    "# train['GeneralPolarity'] = train['GeneralPolarity'].map({0: 0, 1: 1, -1: 2})\n",
    "# train.rename(columns={'RawText': 'text', 'GeneralPolarity':'label'}, inplace=True)\n",
    "# index = train[train['label'] == 1].index\n",
    "# index = np.random.choice(index, size=int(len(index) * 0.7))\n",
    "# train.drop(index=index, inplace=True)\n",
    "# train.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# test = pd.DataFrame(test_datas)[['RawText', 'GeneralPolarity']]\n",
    "# test.dropna(subset=['GeneralPolarity'], inplace=True)\n",
    "# test['GeneralPolarity'] = test['GeneralPolarity'].astype(int)\n",
    "# test['GeneralPolarity'] = test['GeneralPolarity'].map({0: 0, 1: 1, -1: 2})\n",
    "# test.rename(columns={'RawText': 'text', 'GeneralPolarity':'label'}, inplace=True)\n",
    "# test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val, test = train_test_split(test, test_size = .5, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val.reset_index(drop=True, inplace=True) # index 재정렬\n",
    "# test.reset_index(drop=True, inplace=True) # index 재정렬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = Dataset.from_pandas(train[['text', 'label']])\n",
    "# valid_dataset = Dataset.from_pandas(test[['text', 'label']])\n",
    "# test_dataset = Dataset.from_pandas(test[['text', 'label']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load HuggingFace Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from torch import optim\n",
    "from transformers import get_scheduler\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"klue/bert-base\", num_labels=3)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = train_dataset.map(tokenize_function, batched=True).shuffle(seed=42)\n",
    "# valid_dataset = valid_dataset.map(tokenize_function, batched=True).shuffle(seed=42)\n",
    "# test_dataset = test_dataset.map(tokenize_function, batched=True).shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 데이터셋 저장\n",
    "# torch.save(train_dataset, 'dataset/train_dataset.pth')\n",
    "# torch.save(valid_dataset, 'dataset/valid_dataset.pth')\n",
    "# torch.save(test_dataset, 'dataset/test_dataset.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장된 데이터셋 불러오기\n",
    "train_dataset = torch.load('dataset/train_dataset.pth')\n",
    "valid_dataset = torch.load('dataset/valid_dataset.pth')\n",
    "test_dataset = torch.load('dataset/test_dataset.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_num = 50000\n",
    "test_val_num = 10000\n",
    "\n",
    "# 학습이 너무 오래걸리기 때문에 + 데이터가 분류별로 나눠져있어 다양한 데이터를 사용하기 위함\n",
    "small_train_dataset = train_dataset.shuffle(seed=42).select(range(train_num))\n",
    "small_valid_dataset = valid_dataset.shuffle(seed=42).select(range(test_val_num))\n",
    "small_test_dataset = test_dataset.shuffle(seed=42).select(range(test_val_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = small_train_dataset.remove_columns([\"text\"])\n",
    "small_train_dataset = small_train_dataset.rename_column(\"label\", \"labels\")\n",
    "small_train_dataset.set_format(\"torch\") # 텐서로 변환\n",
    "\n",
    "small_valid_dataset = small_valid_dataset.remove_columns([\"text\"])\n",
    "small_valid_dataset = small_valid_dataset.rename_column(\"label\", \"labels\")\n",
    "small_valid_dataset.set_format(\"torch\") # 텐서로 변환\n",
    "\n",
    "small_test_dataset = small_test_dataset.remove_columns([\"text\"])\n",
    "small_test_dataset = small_test_dataset.rename_column(\"label\", \"labels\")\n",
    "small_test_dataset.set_format(\"torch\") # 텐서로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(small_train_dataset, shuffle=True, batch_size=16)\n",
    "valid_dataloader = DataLoader(small_valid_dataset, batch_size=16)\n",
    "test_dataloader = DataLoader(small_test_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyPara = {\n",
    "    \"lr\": 1e-6,\n",
    "    \"epoch\": 10,\n",
    "    \"patience\": 2\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training 코드, evaluation 코드, training_loop 코드\n",
    "def training(model, dataloader, train_dataset, optimizer, scheduler, device, epoch, num_epochs):\n",
    "    model.train()  # 모델을 학습 모드로 설정\n",
    "    train_loss = 0.0\n",
    "    train_accuracy = 0\n",
    "\n",
    "    tbar = tqdm(dataloader)\n",
    "    for batch in tbar:\n",
    "        labels = batch['labels'].to(device)\n",
    "        input_ = batch['input_ids'].to(device)\n",
    "        token_type = batch['token_type_ids'].to(device)\n",
    "        mask = batch['attention_mask'].to(device)\n",
    "\n",
    "        # 순전파\n",
    "        output = model(labels=labels,\n",
    "                    input_ids=input_,\n",
    "                    token_type_ids=token_type,\n",
    "                    attention_mask=mask)\n",
    "\n",
    "        loss = output['loss']\n",
    "\n",
    "        # 역전파 및 가중치 업데이트\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # 손실과 정확도 계산\n",
    "        train_loss += loss.item()\n",
    "        # torch.max에서 dim 인자에 값을 추가할 경우, 해당 dimension에서 최댓값과 최댓값에 해당하는 인덱스를 반환\n",
    "        _, predicted = torch.max(output['logits'], 1)\n",
    "        train_accuracy += (predicted == labels).sum().item()\n",
    "\n",
    "        # tqdm의 진행바에 표시될 설명 텍스트를 설정\n",
    "        tbar.set_description(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # 에폭별 학습 결과 출력\n",
    "    train_loss = train_loss / len(dataloader)\n",
    "    train_accuracy = train_accuracy / len(train_dataset)\n",
    "\n",
    "    return model, train_loss, train_accuracy\n",
    "\n",
    "def evaluation(model, dataloader, val_dataset, device, epoch, num_epochs):\n",
    "    model.eval()  # 모델을 평가 모드로 설정\n",
    "    valid_loss = 0.0\n",
    "    valid_accuracy = 0\n",
    "\n",
    "    with torch.no_grad(): # model의 업데이트 막기\n",
    "        tbar = tqdm(dataloader)\n",
    "        for batch in tbar:\n",
    "            labels = batch['labels'].to(device)\n",
    "            input_ = batch['input_ids'].to(device)\n",
    "            token_type = batch['token_type_ids'].to(device)\n",
    "            mask = batch['attention_mask'].to(device)\n",
    "\n",
    "            # 순전파\n",
    "            output = model(labels=labels,\n",
    "                        input_ids=input_,\n",
    "                        token_type_ids=token_type,\n",
    "                        attention_mask=mask)\n",
    "            \n",
    "            loss = output['loss']\n",
    "            \n",
    "            # 손실과 정확도 계산\n",
    "            valid_loss += loss.item()\n",
    "            # torch.max에서 dim 인자에 값을 추가할 경우, 해당 dimension에서 최댓값과 최댓값에 해당하는 인덱스를 반환\n",
    "            _, predicted = torch.max(output['logits'], 1)\n",
    "            valid_accuracy += (predicted == labels).sum().item()\n",
    "            \n",
    "            # tqdm의 진행바에 표시될 설명 텍스트를 설정\n",
    "            tbar.set_description(f\"Epoch [{epoch+1}/{num_epochs}], Valid Loss: {loss.item():.4f}\")\n",
    "            \n",
    "    valid_loss = valid_loss / len(dataloader)\n",
    "    valid_accuracy = valid_accuracy / len(val_dataset)\n",
    "\n",
    "    return model, valid_loss, valid_accuracy\n",
    "\n",
    "\n",
    "def training_loop(model, train_dataloader, valid_dataloader, train_dataset, val_dataset, optimizer, scheduler, device, num_epochs, patience, model_path):\n",
    "    best_valid_loss = float('inf')  # 가장 좋은 validation loss를 저장\n",
    "    early_stop_counter = 0  # 카운터\n",
    "    valid_max_accuracy = -1\n",
    "    \n",
    "    run = wandb.init(project = 'ko-bert-sentiment02')\n",
    "\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model, train_loss, train_accuracy = training(model, train_dataloader, train_dataset, optimizer, scheduler, device, epoch, num_epochs)\n",
    "        model, valid_loss, valid_accuracy = evaluation(model, valid_dataloader, val_dataset, device, epoch, num_epochs)\n",
    "        \n",
    "        monitoring_value = {'train_num': train_num, 'valid_num':test_val_num,'train_loss': train_loss, 'train_accuracy': train_accuracy, 'valid_loss': valid_loss, 'valid_accuracy': valid_accuracy, 'lr': optimizer.param_groups[0]['initial_lr'], 'lr2': optimizer.param_groups[0]['lr']}\n",
    "        run.log(monitoring_value, step=epoch)\n",
    "        \n",
    "        if valid_accuracy > valid_max_accuracy:\n",
    "            valid_max_accuracy = valid_accuracy\n",
    "            \n",
    "        # validation loss가 감소하면 모델 저장 및 카운터 리셋\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            tokenizer.save_pretrained(model_path)\n",
    "            model.save_pretrained(model_path)\n",
    "            early_stop_counter = 0\n",
    "\n",
    "            \n",
    "        # validation loss가 증가하거나 같으면 카운터 증가\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f} Valid Loss: {valid_loss:.4f}, Valid Accuracy: {valid_accuracy:.4f}\")\n",
    "\n",
    "        # 조기 종료 카운터가 설정한 patience를 초과하면 학습 종료\n",
    "        if early_stop_counter >= patience:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "        \n",
    "    run.finish()\n",
    "    return model, valid_max_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"klue/bert-base\", num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdudcjs2779\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\dudcj\\OneDrive\\Desktop\\AI_Study\\FastCampus\\DeepLearning\\Study01\\ko-bert-base-sentiment-analysis\\wandb\\run-20240107_151957-86wiuwnt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dudcjs2779/ko-bert-sentiment02/runs/86wiuwnt' target=\"_blank\">dainty-meadow-22</a></strong> to <a href='https://wandb.ai/dudcjs2779/ko-bert-sentiment02' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dudcjs2779/ko-bert-sentiment02' target=\"_blank\">https://wandb.ai/dudcjs2779/ko-bert-sentiment02</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dudcjs2779/ko-bert-sentiment02/runs/86wiuwnt' target=\"_blank\">https://wandb.ai/dudcjs2779/ko-bert-sentiment02/runs/86wiuwnt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Train Loss: 1.0788:   0%|          | 7/3125 [00:09<59:23,  1.14s/it]  "
     ]
    }
   ],
   "source": [
    "# 모델 전체 fine tuning\n",
    "model.to(device)\n",
    "num_epochs = hyPara['epoch']\n",
    "model_path = 'my_model01/'\n",
    "lr = hyPara['lr']\n",
    "patience = hyPara['patience']\n",
    "\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=lr, alpha=0.9)\n",
    "\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "scheduler = get_scheduler(\n",
    "    name='cosine', optimizer=optimizer, \n",
    "    num_warmup_steps=0, \n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "model, valid_max_accuracy = training_loop(model, train_dataloader, valid_dataloader, small_train_dataset, small_valid_dataset, optimizer, scheduler, device, num_epochs, patience, model_path)\n",
    "print('Valid max accuracy : ', valid_max_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_evaluation(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_preds = []\n",
    "    total_labels = []\n",
    "    total_probs = []\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            labels = batch['labels'].to(device)\n",
    "            input_ = batch['input_ids'].to(device)\n",
    "            token_type = batch['token_type_ids'].to(device)\n",
    "            mask = batch['attention_mask'].to(device)\n",
    "\n",
    "            output = model(labels=labels,\n",
    "                        input_ids=input_,\n",
    "                        token_type_ids=token_type,\n",
    "                        attention_mask=mask)\n",
    "            \n",
    "            loss = output['loss']\n",
    "            \n",
    "            # 손실과 정확도 계산\n",
    "            valid_loss += loss.item()\n",
    "            \n",
    "            # torch.max에서 dim 인자에 값을 추가할 경우, 해당 dimension에서 최댓값과 최댓값에 해당하는 인덱스를 반환\n",
    "            _, predicted = torch.max(output['logits'], 1)\n",
    "\n",
    "            total_preds.extend(predicted.detach().cpu().tolist())\n",
    "            total_labels.extend(labels.tolist())\n",
    "            total_probs.append(output['logits'].detach().cpu().numpy())\n",
    "            \n",
    "    total_preds = np.array(total_preds)\n",
    "    total_labels = np.array(total_labels)\n",
    "    total_probs = np.concatenate(total_probs, axis= 0)\n",
    "    acc = accuracy_score(total_labels, total_preds)\n",
    "    valid_loss = valid_loss / len(dataloader)\n",
    "    \n",
    "    return acc, valid_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [01:41<00:00,  3.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full fine tuning model accuracy : 0.892, loss: 0.2726\n"
     ]
    }
   ],
   "source": [
    "# tok = AutoTokenizer.from_pretrained(model_path)\n",
    "mod = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "model = mod.to(device)\n",
    "acc, loss = test_evaluation(model, test_dataloader, device)\n",
    "print(f\"Full fine tuning model accuracy : {acc}, loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
