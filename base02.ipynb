{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dudcj\\OneDrive\\Desktop\\AI_Study\\FastCampus\\DeepLearning\\Study01\\ko-bert-base-sentiment-analysis\\.env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from datasets import Dataset, DatasetDict\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "from sklearn.model_selection import train_test_split # train test 를 나누기 위한 라이브러리\n",
    "from sklearn.metrics import accuracy_score # 정확도 계산 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU 메모리 초기화\n",
    "def gpu_clear():\n",
    "    device = cuda.get_current_device(); \n",
    "    device.reset()\n",
    "    !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed 고정\n",
    "import random\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "def random_seed(seed_num):\n",
    "    torch.manual_seed(seed_num)\n",
    "    torch.cuda.manual_seed(seed_num)\n",
    "    torch.cuda.manual_seed_all(seed_num)\n",
    "    np.random.seed(seed_num)\n",
    "    cudnn.benchmark = False\n",
    "    cudnn.deterministic = True\n",
    "    random.seed(seed_num)\n",
    "random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data load for Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_combine_json_files(folder_path):\n",
    "    all_data = []\n",
    "\n",
    "    # 폴더와 하위 폴더를 순회하면서 JSON 파일 찾기\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file_name in files:\n",
    "            if file_name.endswith('.json'):\n",
    "                file_path = os.path.join(root, file_name)\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    json_data = json.load(file)\n",
    "                    all_data.extend(json_data)  # 리스트를 확장하여 데이터 추가\n",
    "\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '../data/train'\n",
    "test_path = '../data/validation'\n",
    "\n",
    "train_datas = load_and_combine_json_files(train_path)\n",
    "test_datas = load_and_combine_json_files(test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.DataFrame(train_datas)[['RawText', 'GeneralPolarity']]\n",
    "train.dropna(subset=['GeneralPolarity'], inplace=True)\n",
    "train['GeneralPolarity'] = train['GeneralPolarity'].astype(int)\n",
    "train['GeneralPolarity'] = train['GeneralPolarity'].map({0: 0, 1: 1, -1: 2})\n",
    "train.rename(columns={'RawText': 'text', 'GeneralPolarity':'label'}, inplace=True)\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "\n",
    "test = pd.DataFrame(test_datas)[['RawText', 'GeneralPolarity']]\n",
    "test.dropna(subset=['GeneralPolarity'], inplace=True)\n",
    "test['GeneralPolarity'] = test['GeneralPolarity'].astype(int)\n",
    "test['GeneralPolarity'] = test['GeneralPolarity'].map({0: 0, 1: 1, -1: 2})\n",
    "test.rename(columns={'RawText': 'text', 'GeneralPolarity':'label'}, inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "val, test = train_test_split(test, test_size = .5, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "val.reset_index(drop=True, inplace=True) # index 재정렬\n",
    "test.reset_index(drop=True, inplace=True) # index 재정렬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_pandas(train[['text', 'label']])\n",
    "valid_dataset = Dataset.from_pandas(test[['text', 'label']])\n",
    "test_dataset = Dataset.from_pandas(test[['text', 'label']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load HuggingFace Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from torch import optim\n",
    "from transformers import get_scheduler\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"klue/bert-base\", num_labels=3)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/183432 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 183432/183432 [01:05<00:00, 2816.35 examples/s]\n",
      "Map: 100%|██████████| 12435/12435 [00:04<00:00, 3013.52 examples/s]\n",
      "Map: 100%|██████████| 12435/12435 [00:04<00:00, 3033.10 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(tokenize_function, batched=True).shuffle(seed=42)\n",
    "valid_dataset = valid_dataset.map(tokenize_function, batched=True).shuffle(seed=42)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True).shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습이 너무 오래걸리기 때문에 + 데이터가 분류별로 나눠져있어 다양한 데이터를 사용하기 위함\n",
    "train_dataset = train_dataset.shuffle(seed=42).select(range(500))\n",
    "valid_dataset = valid_dataset.shuffle(seed=42).select(range(200))\n",
    "test_dataset = test_dataset.shuffle(seed=42).select(range(200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.remove_columns([\"text\"])\n",
    "train_dataset = train_dataset.rename_column(\"label\", \"labels\")\n",
    "train_dataset.set_format(\"torch\") # 텐서로 변환\n",
    "\n",
    "valid_dataset = valid_dataset.remove_columns([\"text\"])\n",
    "valid_dataset = valid_dataset.rename_column(\"label\", \"labels\")\n",
    "valid_dataset.set_format(\"torch\") # 텐서로 변환\n",
    "\n",
    "test_dataset = test_dataset.remove_columns([\"text\"])\n",
    "test_dataset = test_dataset.rename_column(\"label\", \"labels\")\n",
    "test_dataset.set_format(\"torch\") # 텐서로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=8)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=8)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyPara = {\n",
    "    \"lr\": 5e-5,\n",
    "    \"epoch\": 3,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(model.parameters(), lr=hyPara['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = hyPara['epoch']\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu_clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 189/189 [01:31<00:00,  2.05it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "\n",
    "''' \n",
    "train_dataloader는 for문을 돌려서 뽑아보면 dict 타입임\n",
    "batch라는 dict를 for을 돌려 key, value로 하나씩 뽑으면서 v를 gpu에 올리고난뒤 model에서 연산\n",
    "'''\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "progress_bar.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:05<00:00,  4.68it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.855}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "progress_bar = tqdm(range(len(test_dataloader)))\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "model.eval()\n",
    "for batch in test_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "    progress_bar.update(1)\n",
    "    \n",
    "progress_bar.close() \n",
    "\n",
    "metric.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"klue/bert-base\", num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training 코드, evaluation 코드, training_loop 코드\n",
    "def training(model, dataloader, train_dataset, optimizer, device, epoch, num_epochs):\n",
    "    model.train()  # 모델을 학습 모드로 설정\n",
    "    train_loss = 0.0\n",
    "    train_accuracy = 0\n",
    "\n",
    "    tbar = tqdm(dataloader)\n",
    "    for batch in tbar:\n",
    "        labels = batch['labels'].to(device)\n",
    "        input_ = batch['input_ids'].to(device)\n",
    "        token_type = batch['token_type_ids'].to(device)\n",
    "        mask = batch['attention_mask'].to(device)\n",
    "\n",
    "        # 순전파\n",
    "        output = model(labels=labels,\n",
    "                    input_ids=input_,\n",
    "                    token_type_ids=token_type,\n",
    "                    attention_mask=mask)\n",
    "\n",
    "        loss = output['loss'] # 얘 확인\n",
    "\n",
    "        # 역전파 및 가중치 업데이트\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 손실과 정확도 계산\n",
    "        train_loss += loss.item()\n",
    "        # torch.max에서 dim 인자에 값을 추가할 경우, 해당 dimension에서 최댓값과 최댓값에 해당하는 인덱스를 반환\n",
    "        _, predicted = torch.max(output['logits'], 1)\n",
    "        train_accuracy += (predicted == labels).sum().item()\n",
    "\n",
    "        # tqdm의 진행바에 표시될 설명 텍스트를 설정\n",
    "        tbar.set_description(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # 에폭별 학습 결과 출력\n",
    "    train_loss = train_loss / len(dataloader)\n",
    "    train_accuracy = train_accuracy / len(train_dataset)\n",
    "\n",
    "    return model, train_loss, train_accuracy\n",
    "\n",
    "def evaluation(model, dataloader, val_dataset, device, epoch, num_epochs):\n",
    "    model.eval()  # 모델을 평가 모드로 설정\n",
    "    valid_accuracy = 0\n",
    "\n",
    "    with torch.no_grad(): # model의 업데이트 막기\n",
    "        tbar = tqdm(dataloader)\n",
    "        for batch in tbar:\n",
    "            labels = batch['labels'].to(device)\n",
    "            input_ = batch['input_ids'].to(device)\n",
    "            token_type = batch['token_type_ids'].to(device)\n",
    "            mask = batch['attention_mask'].to(device)\n",
    "\n",
    "            # 순전파\n",
    "            output = model(labels=labels,\n",
    "                        input_ids=input_,\n",
    "                        token_type_ids=token_type,\n",
    "                        attention_mask=mask)\n",
    "\n",
    "            # torch.max에서 dim 인자에 값을 추가할 경우, 해당 dimension에서 최댓값과 최댓값에 해당하는 인덱스를 반환\n",
    "            _, predicted = torch.max(output['logits'], 1)\n",
    "            valid_accuracy += (predicted == labels).sum().item()\n",
    "\n",
    "            # tqdm의 진행바에 표시될 설명 텍스트를 설정\n",
    "            tbar.set_description(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "\n",
    "    valid_accuracy = valid_accuracy / len(val_dataset)\n",
    "\n",
    "    return model, valid_accuracy\n",
    "\n",
    "\n",
    "def training_loop(model, train_dataloader, valid_dataloader, train_dataset, val_dataset, optimizer, device, num_epochs, model_path):\n",
    "    best_valid_loss = float('inf')  # 가장 좋은 validation loss를 저장\n",
    "    valid_max_accuracy = -1\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model, train_loss, train_accuracy = training(model, train_dataloader, train_dataset, optimizer, device, epoch, num_epochs)\n",
    "        model, valid_accuracy = evaluation(model, valid_dataloader, val_dataset, device, epoch, num_epochs)\n",
    "\n",
    "        if valid_accuracy > valid_max_accuracy:\n",
    "            valid_max_accuracy = valid_accuracy\n",
    "            tokenizer.save_pretrained(model_path)\n",
    "            model.save_pretrained(model_path)\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Valid Accuracy: {valid_accuracy:.4f}\")\n",
    "\n",
    "    return model, valid_max_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Train Loss: 0.7832: 100%|██████████| 63/63 [00:31<00:00,  2.03it/s]\n",
      "Epoch [1/3]: 100%|██████████| 25/25 [00:04<00:00,  6.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Train Loss: 0.5616, Train Accuracy: 0.7620, Valid Accuracy: 0.8400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [2/3], Train Loss: 0.0607: 100%|██████████| 63/63 [00:30<00:00,  2.04it/s]\n",
      "Epoch [2/3]: 100%|██████████| 25/25 [00:04<00:00,  6.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/3], Train Loss: 0.2764, Train Accuracy: 0.8960, Valid Accuracy: 0.8600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [3/3], Train Loss: 0.0287: 100%|██████████| 63/63 [00:30<00:00,  2.05it/s]\n",
      "Epoch [3/3]: 100%|██████████| 25/25 [00:04<00:00,  6.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/3], Train Loss: 0.1261, Train Accuracy: 0.9540, Valid Accuracy: 0.8000\n",
      "Valid max accuracy :  0.86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 모델 전체 fine tuning\n",
    "model.to(device)\n",
    "num_epochs = hyPara['epoch']\n",
    "model_path = 'my_model01/'\n",
    "lr = hyPara['lr']\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "model, valid_max_accuracy = training_loop(model, train_dataloader, valid_dataloader, train_dataset, valid_dataset, optimizer, device, num_epochs, model_path)\n",
    "print('Valid max accuracy : ', valid_max_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_evaluation(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_preds = []\n",
    "    total_labels = []\n",
    "    total_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            labels = batch['labels'].to(device)\n",
    "            input_ = batch['input_ids'].to(device)\n",
    "            token_type = batch['token_type_ids'].to(device)\n",
    "            mask = batch['attention_mask'].to(device)\n",
    "\n",
    "            output = model(labels=labels,\n",
    "                        input_ids=input_,\n",
    "                        token_type_ids=token_type,\n",
    "                        attention_mask=mask)\n",
    "\n",
    "\n",
    "            # torch.max에서 dim 인자에 값을 추가할 경우, 해당 dimension에서 최댓값과 최댓값에 해당하는 인덱스를 반환\n",
    "            _, predicted = torch.max(output['logits'], 1)\n",
    "\n",
    "            total_preds.extend(predicted.detach().cpu().tolist())\n",
    "            total_labels.extend(labels.tolist())\n",
    "            total_probs.append(output['logits'].detach().cpu().numpy())\n",
    "\n",
    "    total_preds = np.array(total_preds)\n",
    "    total_labels = np.array(total_labels)\n",
    "    total_probs = np.concatenate(total_probs, axis= 0)\n",
    "    acc = accuracy_score(total_labels, total_preds)\n",
    "    return acc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:04<00:00,  6.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full fine tuning model accuracy :  0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# tok = AutoTokenizer.from_pretrained(model_path)\n",
    "mod = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "model = model.to(device)\n",
    "acc = test_evaluation(model, test_dataloader, device)\n",
    "print(\"Full fine tuning model accuracy : \",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
